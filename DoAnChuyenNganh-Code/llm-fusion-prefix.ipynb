{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12409189,"sourceType":"datasetVersion","datasetId":7825921},{"sourceId":12409190,"sourceType":"datasetVersion","datasetId":7825944},{"sourceId":12409622,"sourceType":"datasetVersion","datasetId":7826241}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"### Cell 1: Cài đặt và import\n!pip install transformers datasets peft bitsandbytes accelerate scikit-learn\n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\nfrom peft import get_peft_model, PrefixTuningConfig\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\n\ndef free_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T09:37:56.687271Z","iopub.execute_input":"2025-07-09T09:37:56.687560Z","iopub.status.idle":"2025-07-09T09:38:00.298950Z","shell.execute_reply.started":"2025-07-09T09:37:56.687540Z","shell.execute_reply":"2025-07-09T09:38:00.298036Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"### Cell 2: Load CSV & Embeddings, Merge\n# CSV paths\ncsv_paths = {\n    'train': '/kaggle/input/id-data/train.csv',\n    'validation': '/kaggle/input/id-data/validation.csv',\n    'test': '/kaggle/input/id-data/test.csv'\n}\ncsv_dfs = {s: pd.read_csv(p)[['id','code','label']].dropna() for s,p in csv_paths.items()}\n\n# Embedding paths\nemb_paths = {\n    'code': {\n        'train': '/kaggle/input/emb-token/train_emb.pt',\n        'validation': '/kaggle/input/emb-token/val_emb.pt',\n        'test': '/kaggle/input/emb-token/test_emb.pt'\n    },\n    'pdg': {\n        'train': '/kaggle/input/emb-pdg/train_emb.pt',\n        'validation': '/kaggle/input/emb-pdg/val_emb.pt',\n        'test': '/kaggle/input/emb-pdg/test_emb.pt'\n    }\n}\n\nmerged = {}\nfor split in ['train','validation','test']:\n    code_data = torch.load(emb_paths['code'][split])\n    pdg_data  = torch.load(emb_paths['pdg'][split])\n    df_code = pd.DataFrame({'id': code_data['ids'], 'code_emb': [e.cpu() for e in code_data['embeddings']]})\n    df_pdg  = pd.DataFrame({'id': pdg_data['ids'],  'pdg_emb':  [e.cpu() for e in pdg_data['embeddings']]})\n    df = csv_dfs[split].merge(df_code, on='id').merge(df_pdg, on='id')\n    merged[split] = df.reset_index(drop=True)\n    print(f\"{split}: {len(df)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T09:38:00.300907Z","iopub.execute_input":"2025-07-09T09:38:00.301185Z","iopub.status.idle":"2025-07-09T09:38:19.707728Z","shell.execute_reply.started":"2025-07-09T09:38:00.301164Z","shell.execute_reply":"2025-07-09T09:38:19.706921Z"}},"outputs":[{"name":"stdout","text":"train: 75909 samples\nvalidation: 16189 samples\ntest: 16175 samples\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"### Cell 3: Tạo DatasetDict cho prefix-tuning\n# Build DatasetDict with code text and PDG embeddings\nraw = DatasetDict({\n    split: Dataset.from_dict({\n        'code': merged[split]['code'].tolist(),\n        'pdg_emb': merged[split]['pdg_emb'].apply(lambda x: x.tolist()).tolist(),\n        'label': merged[split]['label'].tolist()\n    }) for split in merged\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T09:38:19.708676Z","iopub.execute_input":"2025-07-09T09:38:19.708944Z","iopub.status.idle":"2025-07-09T09:38:42.424898Z","shell.execute_reply.started":"2025-07-09T09:38:19.708924Z","shell.execute_reply":"2025-07-09T09:38:42.424107Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"### Cell 4: Tokenize code and attach pdg_emb\nmodel_ckpt = 'Qwen/Qwen2.5-Coder-0.5B-Instruct'\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n# Ensure a pad token is defined for batch padding\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\nmax_length = 512\n\ndef preprocess(examples):\n    toks = tokenizer(\n        examples['code'], truncation=True,\n        padding='max_length', max_length=max_length\n    )\n    toks['pdg_emb'] = examples['pdg_emb']\n    toks['labels']  = examples['label']\n    return toks\n\ndatasets = raw.map(preprocess, batched=True)\n# set torch format\ndatasets.set_format(\n    type='torch',\n    columns=['input_ids','attention_mask','pdg_emb','labels']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T09:54:34.318165Z","iopub.execute_input":"2025-07-09T09:54:34.318842Z","iopub.status.idle":"2025-07-09T09:58:55.877927Z","shell.execute_reply.started":"2025-07-09T09:54:34.318809Z","shell.execute_reply":"2025-07-09T09:58:55.877377Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/75909 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3244347b69643479d9f309f5c20b048"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16189 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3cfa359167d45209d9ce9960b376bcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16175 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a45d67a5619241029d20c79b5e9fbd9a"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"### Cell 5: Thiết lập PEFT Prefix-Tuning trên Qwen2.5\n# Set pad_token_id in model config\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    model_ckpt,\n    num_labels=len(set(merged['train']['label'])),\n    torch_dtype=torch.float16\n)\nbase_model.config.pad_token_id = tokenizer.pad_token_id\nbase_model.cuda()\n\n# Prefix tuning config\nprefix_len = 8\npeft_config = PrefixTuningConfig(\n    task_type='SEQ_CLS',\n    prefix_projection=True,\n    num_virtual_tokens=prefix_len\n)\n# Wrap model\nmodel = get_peft_model(base_model, peft_config)\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T10:03:50.987586Z","iopub.execute_input":"2025-07-09T10:03:50.987900Z","iopub.status.idle":"2025-07-09T10:03:52.516825Z","shell.execute_reply.started":"2025-07-09T10:03:50.987869Z","shell.execute_reply":"2025-07-09T10:03:52.516234Z"}},"outputs":[{"name":"stderr","text":"Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-Coder-0.5B-Instruct and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"### Cell 6: TrainingArguments và Trainer\ntraining_args = TrainingArguments(\n    output_dir='./qwen_prefix',\n    learning_rate=3e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    fp16=True,\n    logging_steps=100,\n    report_to='none'\n)\n\ndef compute_metrics(p):\n    preds = p.predictions.argmax(-1)\n    acc = accuracy_score(p.label_ids, preds)\n    prec, rec, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='macro')\n    return {'accuracy':acc,'precision':prec,'recall':rec,'f1':f1}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=datasets['train'],\n    eval_dataset=datasets['validation'],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\nfree_memory()\ntrainer.train()\nfree_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T10:07:52.121493Z","iopub.execute_input":"2025-07-09T10:07:52.122207Z","iopub.status.idle":"2025-07-09T19:07:08.111625Z","shell.execute_reply.started":"2025-07-09T10:07:52.122181Z","shell.execute_reply":"2025-07-09T19:07:08.111041Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1340052381.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14235' max='14235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14235/14235 8:59:11, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.408500</td>\n      <td>0.369442</td>\n      <td>0.832973</td>\n      <td>0.773205</td>\n      <td>0.716476</td>\n      <td>0.737262</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.322500</td>\n      <td>0.302640</td>\n      <td>0.871703</td>\n      <td>0.837054</td>\n      <td>0.778286</td>\n      <td>0.801536</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.309600</td>\n      <td>0.287149</td>\n      <td>0.880351</td>\n      <td>0.849132</td>\n      <td>0.794067</td>\n      <td>0.816451</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"### Cell 7: Đánh giá trên test\nres = trainer.predict(datasets['test'])\nprint(res.metrics)\npreds = res.predictions.argmax(-1)\nprint(classification_report(res.label_ids, preds, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:07:25.194722Z","iopub.execute_input":"2025-07-09T19:07:25.195009Z","iopub.status.idle":"2025-07-09T19:23:12.603813Z","shell.execute_reply.started":"2025-07-09T19:07:25.194970Z","shell.execute_reply":"2025-07-09T19:23:12.603246Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'test_loss': 0.2997478246688843, 'test_accuracy': 0.874435857805255, 'test_precision': 0.8383769905591381, 'test_recall': 0.7867517971317355, 'test_f1': 0.8078438605889944, 'test_runtime': 947.3624, 'test_samples_per_second': 17.074, 'test_steps_per_second': 2.134}\n              precision    recall  f1-score   support\n\n           0     0.8952    0.9482    0.9210     12479\n           1     0.7815    0.6253    0.6947      3696\n\n    accuracy                         0.8744     16175\n   macro avg     0.8384    0.7868    0.8078     16175\nweighted avg     0.8692    0.8744    0.8693     16175\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"### Cell 8: Lưu model và tokenizer\n# Lưu state PEFT\nmodel.save_pretrained('./qwen2_prefix_tuned')\n# tokenizer\ntokenizer.save_pretrained('./qwen2_prefix_tuned')\n# Nếu muốn lưu cả base_model config và weights\nbase_model.save_pretrained('./qwen2_prefix_tuned_base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:23:12.604917Z","iopub.execute_input":"2025-07-09T19:23:12.605217Z","iopub.status.idle":"2025-07-09T19:23:15.039021Z","shell.execute_reply.started":"2025-07-09T19:23:12.605191Z","shell.execute_reply":"2025-07-09T19:23:15.038427Z"}},"outputs":[],"execution_count":22}]}