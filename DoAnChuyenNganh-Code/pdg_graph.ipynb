{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12427763,"sourceType":"datasetVersion","datasetId":7838829}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: CÃ i Ä‘áº·t mÃ´i trÆ°á»ng vÃ  import thÆ° viá»‡n\n# CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t\n!pip install -q torch torchvision torchaudio\n!pip install -q torch-geometric\n!pip install -q transformers\n!pip install -q scikit-learn tqdm pandas\n\n# Import thÆ° viá»‡n cÆ¡ báº£n\nimport os\nimport ast\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# PyTorch & Torch Geometric\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import RGCNConv, global_mean_pool\n\n# Transformers - dÃ¹ng CodeBERT\nfrom transformers import AutoTokenizer, AutoModel\n\n# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score,\n    classification_report\n)\n\n# CÃ i Ä‘áº·t thiáº¿t bá»‹\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"ğŸ”§ Äang sá»­ dá»¥ng thiáº¿t bá»‹: {device}\")\n\n# TrÃ¡nh cáº£nh bÃ¡o khÃ´ng cáº§n thiáº¿t tá»« tokenizer\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T07:14:37.741241Z","iopub.execute_input":"2025-07-10T07:14:37.741528Z","iopub.status.idle":"2025-07-10T07:16:09.290620Z","shell.execute_reply.started":"2025-07-10T07:14:37.741505Z","shell.execute_reply":"2025-07-10T07:16:09.289845Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hğŸ”§ Äang sá»­ dá»¥ng thiáº¿t bá»‹: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Chuáº©n bá»‹ dataset (load CSV + trÃ­ch xuáº¥t PDG)\nimport ast\n\n# Äá»c vÃ  lÃ m sáº¡ch dá»¯ liá»‡u tá»« CSV\ndef prepare_datasets(csv_paths: dict):\n    datasets = {}\n    for split, path in csv_paths.items():\n        df = pd.read_csv(path)\n        df = df[df['code'].notna() & df['label'].isin([0, 1])].reset_index(drop=True)\n        datasets[split] = df\n        print(f\"{split.upper()} set: {df.shape[0]} samples\")\n    return datasets\n\n# TrÃ­ch xuáº¥t PDG: tráº£ vá» nodes, edge_index, edge_attr\ndef extract_pdg(code):\n    try:\n        tree = ast.parse(code)\n        nodes, edges, var_deps = [], [], {}\n\n        def visit(node, parent_idx=None):\n            idx = len(nodes)\n            node_str = ast.unparse(node).strip() if hasattr(ast, \"unparse\") else str(node)\n            nodes.append(node_str)\n\n            if parent_idx is not None:\n                edges.append((parent_idx, idx, 'control'))\n\n            if isinstance(node, ast.Assign):\n                for target in node.targets:\n                    if isinstance(target, ast.Name):\n                        var_name = target.id\n                        var_deps.setdefault(var_name, []).append(idx)\n                        for sub in ast.walk(node.value):\n                            if isinstance(sub, ast.Name) and sub.id in var_deps:\n                                for dep_idx in var_deps[sub.id]:\n                                    edges.append((dep_idx, idx, 'data'))\n            elif isinstance(node, ast.Name):\n                var_name = node.id\n                if var_name in var_deps:\n                    for dep_idx in var_deps[var_name]:\n                        edges.append((dep_idx, idx, 'data'))\n\n            for child in ast.iter_child_nodes(node):\n                visit(child, idx)\n\n        visit(tree)\n        edge_index = torch.tensor([(u, v) for u, v, _ in edges], dtype=torch.long).t()\n        edge_attr = torch.tensor([[1, 0] if t == 'control' else [0, 1] for _, _, t in edges], dtype=torch.float)\n        return nodes, edge_index, edge_attr\n    except Exception:\n        return [], torch.empty((2, 0), dtype=torch.long), torch.empty((0, 2), dtype=torch.float)\n\n# Táº¡o bá»™ dataset dáº¡ng list chá»©a (nodes, edge_index, edge_attr, label)\ndef build_raw_pdg_dataset(df, split_name):\n    raw_dataset = []\n    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"ğŸ“¦ Building PDG for {split_name}\"):\n        nodes, edge_index, edge_attr = extract_pdg(row['code'])\n        if len(nodes) > 0 and edge_index.size(1) > 0:\n            raw_dataset.append((nodes, edge_index, edge_attr, int(row['label'])))\n    print(f\"âœ… {split_name} set: {len(raw_dataset)} valid samples (PDG extracted)\")\n    return raw_dataset\n\n# ÄÆ°á»ng dáº«n tá»›i file CSV gá»‘c\ncsv_files = {\n    'train': \"/kaggle/input/half1-1/train.csv\",\n    'val': \"/kaggle/input/half1-1/val.csv\",\n    'test': \"/kaggle/input/half1-1/test.csv\"\n}\n\n# Load dá»¯ liá»‡u CSV & PDG\ndatasets = prepare_datasets(csv_files)\nraw_dataset_dict = {split: build_raw_pdg_dataset(datasets[split], split) for split in ['train', 'val', 'test']}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T07:16:09.291877Z","iopub.execute_input":"2025-07-10T07:16:09.292326Z","iopub.status.idle":"2025-07-10T07:23:41.805903Z","shell.execute_reply.started":"2025-07-10T07:16:09.292302Z","shell.execute_reply":"2025-07-10T07:23:41.805135Z"}},"outputs":[{"name":"stdout","text":"TRAIN set: 22256 samples\nVAL set: 4769 samples\nTEST set: 4770 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"ğŸ“¦ Building PDG for train:   0%|          | 0/22256 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e993105f1b6341c1a5929e88803a1216"}},"metadata":{}},{"name":"stdout","text":"âœ… train set: 18936 valid samples (PDG extracted)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"ğŸ“¦ Building PDG for val:   0%|          | 0/4769 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3346b228e55a48e78f3f97440554e86e"}},"metadata":{}},{"name":"stdout","text":"âœ… val set: 4089 valid samples (PDG extracted)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"ğŸ“¦ Building PDG for test:   0%|          | 0/4770 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d7de674135b4664a5a321065143b6df"}},"metadata":{}},{"name":"stdout","text":"âœ… test set: 4039 valid samples (PDG extracted)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Táº¡o node embedding báº±ng CodeBERT & chuáº©n bá»‹ Data cho GNN\nfrom transformers import AutoTokenizer, AutoModel\n\n# âœ… Load CodeBERT\nmodel_id = \"microsoft/codebert-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModel.from_pretrained(model_id).to(device).eval()\n\nMAX_NODES = 100\n\n# âœ… HÃ m táº¡o embedding cho list node báº±ng CodeBERT\ndef create_node_embeddings(nodes, batch_size=64):\n    all_embeddings = []\n    for i in range(0, len(nodes), batch_size):\n        batch_nodes = nodes[i:i + batch_size]\n        inputs = tokenizer(batch_nodes, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        embeddings = outputs.last_hidden_state.mean(dim=1).cpu()  # (B, 768)\n        all_embeddings.append(embeddings)\n    return torch.cat(all_embeddings, dim=0)\n\n# âœ… Chuyá»ƒn 1 máº«u PDG â†’ Data\ndef process_sample(nodes, edge_index, edge_attr, label):\n    if len(nodes) == 0 or edge_index.size(1) == 0:\n        return None\n    if len(nodes) > MAX_NODES:\n        nodes = nodes[:MAX_NODES]\n        mask = (edge_index[0] < MAX_NODES) & (edge_index[1] < MAX_NODES)\n        edge_index = edge_index[:, mask]\n        edge_attr = edge_attr[mask]\n    x = create_node_embeddings(nodes)  # (num_nodes, 768)\n    y = torch.tensor([label], dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n\n# âœ… Táº¡o embedded data cho táº¥t cáº£ cÃ¡c táº­p\nembedded_dataset_dict = {}\nfor split in ['train', 'val', 'test']:\n    embedded_data = []\n    for i, (nodes, edge_index, edge_attr, label) in enumerate(tqdm(raw_dataset_dict[split], desc=f\"ğŸ” Embedding {split}\")):\n        data = process_sample(nodes, edge_index, edge_attr, label)\n        if data is not None:\n            embedded_data.append(data)\n        if i % 100 == 0:\n            torch.cuda.empty_cache()\n    embedded_dataset_dict[split] = embedded_data\n\n# âœ… Kiá»ƒm tra 1 máº«u\nif len(embedded_dataset_dict['train']) > 0:\n    sample = embedded_dataset_dict['train'][0]\n    print(\"ğŸ“Œ Sample shape:\")\n    print(\"x:\", sample.x.shape)\n    print(\"edge_index:\", sample.edge_index.shape)\n    print(\"edge_attr:\", sample.edge_attr.shape)\n    print(\"label:\", sample.y.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T07:23:41.806676Z","iopub.execute_input":"2025-07-10T07:23:41.806942Z","iopub.status.idle":"2025-07-10T08:30:19.720762Z","shell.execute_reply.started":"2025-07-10T07:23:41.806919Z","shell.execute_reply":"2025-07-10T08:30:19.719939Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86bcd441b0924ea3a71a4841a4a3b0ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef9508b76d3464390c1706badb11f5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cbdb9933f7a4830abd0495a8e4c120d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97d6b4e034cc4097a8ba609e7500034a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618a15157e6b40179a9fdd5bc2efdf97"}},"metadata":{}},{"name":"stderr","text":"2025-07-10 07:23:51.010411: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752132231.195539      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752132231.246078      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2751913bd834e6899b518bd0bc5f6d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12288ad7eeae48ebba11b34d681dd3ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ğŸ” Embedding train:   0%|          | 0/18936 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b9f03094724348ba21569fd8024d4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ğŸ” Embedding val:   0%|          | 0/4089 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796bbd858b8a4f57a74f844f9baae2a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ğŸ” Embedding test:   0%|          | 0/4039 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d890203da6b49bcb40e905a863e0f62"}},"metadata":{}},{"name":"stdout","text":"ğŸ“Œ Sample shape:\nx: torch.Size([100, 768])\nedge_index: torch.Size([2, 102])\nedge_attr: torch.Size([102, 2])\nlabel: 0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 4: Load DataLoader cho GNN tá»« biáº¿n RAM\nfrom torch_geometric.loader import DataLoader\n\n# Load tá»« embedded_dataset_dict\ntrain_data = embedded_dataset_dict['train']\nval_data = embedded_dataset_dict['val']\ntest_data = embedded_dataset_dict['test']\n\n# Táº¡o DataLoader\nbatch_size = 32\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size)\ntest_loader = DataLoader(test_data, batch_size=batch_size)\n\nprint(f\"âœ… Loaded: {len(train_data)} train | {len(val_data)} val | {len(test_data)} test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T08:30:19.722396Z","iopub.execute_input":"2025-07-10T08:30:19.723006Z","iopub.status.idle":"2025-07-10T08:30:19.728557Z","shell.execute_reply.started":"2025-07-10T08:30:19.722988Z","shell.execute_reply":"2025-07-10T08:30:19.727961Z"}},"outputs":[{"name":"stdout","text":"âœ… Loaded: 18936 train | 4089 val | 4039 test\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from torch_geometric.nn import RGCNConv\nfrom torch_geometric.nn.aggr import AttentionalAggregation as NewAttentionAgg\nfrom torch.optim import AdamW\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score,\n    precision_recall_fscore_support\n)\nfrom torch.nn import LayerNorm, Dropout\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom collections import Counter\n\n# âœ… Focal Loss vá»›i gamma=1.5\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=1.5, weight=None, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.weight = weight\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n\n# âœ… RGCN vá»›i Attention + Dropout 0.6\nclass PDG_RGCN_Attention(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_relations=2):\n        super().__init__()\n        self.conv1 = RGCNConv(input_dim, hidden_dim, num_relations)\n        self.norm1 = LayerNorm(hidden_dim)\n\n        self.conv2 = RGCNConv(hidden_dim, hidden_dim, num_relations)\n        self.norm2 = LayerNorm(hidden_dim)\n\n        self.dropout = Dropout(0.6)  # ğŸ”¼ tÄƒng lÃªn 0.6\n\n        gate_nn = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, 1)\n        )\n        self.attention_pool = NewAttentionAgg(gate_nn, nn.Identity())\n\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.classifier_dropout = Dropout(0.6)  # ğŸ”¼ tÄƒng lÃªn 0.6\n        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        edge_type = edge_attr.argmax(dim=1)\n\n        x = F.relu(self.norm1(self.conv1(x, edge_index, edge_type)))\n        x = self.dropout(x)\n\n        x = F.relu(self.norm2(self.conv2(x, edge_index, edge_type)))\n        x = self.dropout(x)\n\n        x = self.attention_pool(x, batch)\n        x = F.relu(self.fc1(x))\n        x = self.classifier_dropout(x)\n        return self.fc2(x)\n\n# âœ… Thiáº¿t láº­p huáº¥n luyá»‡n\nlabels = [data.y.item() for data in train_data]\nlabel_counts = Counter(labels)\ntotal = sum(label_counts.values())\nclass_weights = [total / label_counts[i] for i in range(2)]\nweights_tensor = torch.tensor(class_weights).to(device)\n\ncriterion = FocalLoss(alpha=1.0, gamma=1.5, weight=weights_tensor)\n\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=64)\n\ninput_dim = train_data[0].x.shape[1]\nmodel = PDG_RGCN_Attention(input_dim, hidden_dim=512, output_dim=2, num_relations=2).to(device)\noptimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=3e-5)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n\n# âœ… HÃ m huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ giá»¯ nguyÃªn\ndef train_epoch(model, loader):\n    model.train()\n    total_loss = 0\n    for batch in loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)\n        loss = criterion(out, batch.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef evaluate(model, loader):\n    model.eval()\n    preds, probs, labels = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = batch.to(device)\n            out = model(batch)\n            prob = F.softmax(out, dim=1)[:, 1].cpu()\n            pred = out.argmax(dim=1).cpu()\n            preds.extend(pred.tolist())\n            probs.extend(prob.tolist())\n            labels.extend(batch.y.cpu().tolist())\n\n    if len(set(labels)) < 2:\n        print(\"âš ï¸ Chá»‰ cÃ³ 1 lá»›p trong dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡.\")\n        return accuracy_score(labels, preds), 0, 0, f1_score(labels, preds, zero_division=0), 0.5\n\n    acc = accuracy_score(labels, preds)\n    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n    auc = roc_auc_score(labels, probs)\n    return acc, p, r, f1, auc\n\n# âœ… Huáº¥n luyá»‡n vá»›i early stopping\nbest_f1 = 0\npatience = 12\npatience_counter = 0\nSAVE_MODEL = True\n\nfor epoch in range(1, 101):\n    loss = train_epoch(model, train_loader)\n    scheduler.step()\n\n    val_acc, val_p, val_r, val_f1, val_auc = evaluate(model, val_loader)\n    print(f\"ğŸ“˜ Epoch {epoch:02d} | Loss: {loss:.4f} | Acc: {val_acc:.4f} | P: {val_p:.4f} | R: {val_r:.4f} | F1: {val_f1:.4f} | AUC: {val_auc:.4f}\")\n\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        patience_counter = 0\n        if SAVE_MODEL:\n            torch.save(model.state_dict(), \"best_model.pt\")\n            print(f\"âœ¨ Model saved with F1-score: {best_f1:.4f}\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"ğŸ›‘ Early stopping triggered.\")\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:29:22.588800Z","iopub.execute_input":"2025-07-10T10:29:22.589378Z","iopub.status.idle":"2025-07-10T10:40:03.601373Z","shell.execute_reply.started":"2025-07-10T10:29:22.589354Z","shell.execute_reply":"2025-07-10T10:40:03.600607Z"}},"outputs":[{"name":"stdout","text":"ğŸ“˜ Epoch 01 | Loss: 0.9207 | Acc: 0.5265 | P: 0.3030 | R: 0.8391 | F1: 0.4453 | AUC: 0.7309\nâœ¨ Model saved with F1-score: 0.4453\nğŸ“˜ Epoch 02 | Loss: 0.8424 | Acc: 0.6202 | P: 0.3456 | R: 0.7581 | F1: 0.4748 | AUC: 0.7496\nâœ¨ Model saved with F1-score: 0.4748\nğŸ“˜ Epoch 03 | Loss: 0.8221 | Acc: 0.6598 | P: 0.3699 | R: 0.7138 | F1: 0.4873 | AUC: 0.7601\nâœ¨ Model saved with F1-score: 0.4873\nğŸ“˜ Epoch 04 | Loss: 0.8048 | Acc: 0.5644 | P: 0.3274 | R: 0.8758 | F1: 0.4766 | AUC: 0.7735\nğŸ“˜ Epoch 05 | Loss: 0.7885 | Acc: 0.4984 | P: 0.3046 | R: 0.9471 | F1: 0.4610 | AUC: 0.7813\nğŸ“˜ Epoch 06 | Loss: 0.7841 | Acc: 0.6346 | P: 0.3639 | R: 0.8197 | F1: 0.5040 | AUC: 0.7825\nâœ¨ Model saved with F1-score: 0.5040\nğŸ“˜ Epoch 07 | Loss: 0.7715 | Acc: 0.6185 | P: 0.3549 | R: 0.8369 | F1: 0.4984 | AUC: 0.7841\nğŸ“˜ Epoch 08 | Loss: 0.7691 | Acc: 0.5647 | P: 0.3296 | R: 0.8920 | F1: 0.4814 | AUC: 0.7878\nğŸ“˜ Epoch 09 | Loss: 0.7662 | Acc: 0.5070 | P: 0.3081 | R: 0.9449 | F1: 0.4647 | AUC: 0.7927\nğŸ“˜ Epoch 10 | Loss: 0.7546 | Acc: 0.6505 | P: 0.3765 | R: 0.8283 | F1: 0.5177 | AUC: 0.7972\nâœ¨ Model saved with F1-score: 0.5177\nğŸ“˜ Epoch 11 | Loss: 0.7401 | Acc: 0.6970 | P: 0.4084 | R: 0.7538 | F1: 0.5298 | AUC: 0.7990\nâœ¨ Model saved with F1-score: 0.5298\nğŸ“˜ Epoch 12 | Loss: 0.7300 | Acc: 0.5852 | P: 0.3423 | R: 0.9028 | F1: 0.4964 | AUC: 0.8018\nğŸ“˜ Epoch 13 | Loss: 0.7269 | Acc: 0.6082 | P: 0.3536 | R: 0.8812 | F1: 0.5046 | AUC: 0.8050\nğŸ“˜ Epoch 14 | Loss: 0.7293 | Acc: 0.6989 | P: 0.4112 | R: 0.7624 | F1: 0.5342 | AUC: 0.8044\nâœ¨ Model saved with F1-score: 0.5342\nğŸ“˜ Epoch 15 | Loss: 0.7242 | Acc: 0.6787 | P: 0.3971 | R: 0.8089 | F1: 0.5327 | AUC: 0.8097\nğŸ“˜ Epoch 16 | Loss: 0.7271 | Acc: 0.6923 | P: 0.4077 | R: 0.7916 | F1: 0.5382 | AUC: 0.8086\nâœ¨ Model saved with F1-score: 0.5382\nğŸ“˜ Epoch 17 | Loss: 0.7124 | Acc: 0.6703 | P: 0.3913 | R: 0.8207 | F1: 0.5300 | AUC: 0.8094\nğŸ“˜ Epoch 18 | Loss: 0.7124 | Acc: 0.6610 | P: 0.3858 | R: 0.8391 | F1: 0.5286 | AUC: 0.8146\nğŸ“˜ Epoch 19 | Loss: 0.7049 | Acc: 0.7513 | P: 0.4668 | R: 0.6901 | F1: 0.5569 | AUC: 0.8176\nâœ¨ Model saved with F1-score: 0.5569\nğŸ“˜ Epoch 20 | Loss: 0.7096 | Acc: 0.7016 | P: 0.4162 | R: 0.7883 | F1: 0.5448 | AUC: 0.8187\nğŸ“˜ Epoch 21 | Loss: 0.6852 | Acc: 0.7356 | P: 0.4489 | R: 0.7354 | F1: 0.5575 | AUC: 0.8225\nâœ¨ Model saved with F1-score: 0.5575\nğŸ“˜ Epoch 22 | Loss: 0.6879 | Acc: 0.6356 | P: 0.3719 | R: 0.8844 | F1: 0.5237 | AUC: 0.8234\nğŸ“˜ Epoch 23 | Loss: 0.6811 | Acc: 0.7520 | P: 0.4691 | R: 0.7203 | F1: 0.5681 | AUC: 0.8274\nâœ¨ Model saved with F1-score: 0.5681\nğŸ“˜ Epoch 24 | Loss: 0.6829 | Acc: 0.7606 | P: 0.4804 | R: 0.7019 | F1: 0.5704 | AUC: 0.8277\nâœ¨ Model saved with F1-score: 0.5704\nğŸ“˜ Epoch 25 | Loss: 0.6762 | Acc: 0.7479 | P: 0.4633 | R: 0.7160 | F1: 0.5626 | AUC: 0.8257\nğŸ“˜ Epoch 26 | Loss: 0.6755 | Acc: 0.7897 | P: 0.5301 | R: 0.6285 | F1: 0.5751 | AUC: 0.8298\nâœ¨ Model saved with F1-score: 0.5751\nğŸ“˜ Epoch 27 | Loss: 0.6694 | Acc: 0.7058 | P: 0.4229 | R: 0.8207 | F1: 0.5582 | AUC: 0.8354\nğŸ“˜ Epoch 28 | Loss: 0.6611 | Acc: 0.7525 | P: 0.4700 | R: 0.7268 | F1: 0.5708 | AUC: 0.8343\nğŸ“˜ Epoch 29 | Loss: 0.6626 | Acc: 0.7310 | P: 0.4458 | R: 0.7732 | F1: 0.5656 | AUC: 0.8359\nğŸ“˜ Epoch 30 | Loss: 0.6569 | Acc: 0.7400 | P: 0.4571 | R: 0.7873 | F1: 0.5783 | AUC: 0.8406\nâœ¨ Model saved with F1-score: 0.5783\nğŸ“˜ Epoch 31 | Loss: 0.6446 | Acc: 0.7031 | P: 0.4224 | R: 0.8467 | F1: 0.5636 | AUC: 0.8442\nğŸ“˜ Epoch 32 | Loss: 0.6390 | Acc: 0.7417 | P: 0.4595 | R: 0.7970 | F1: 0.5829 | AUC: 0.8474\nâœ¨ Model saved with F1-score: 0.5829\nğŸ“˜ Epoch 33 | Loss: 0.6359 | Acc: 0.7777 | P: 0.5065 | R: 0.7160 | F1: 0.5933 | AUC: 0.8450\nâœ¨ Model saved with F1-score: 0.5933\nğŸ“˜ Epoch 34 | Loss: 0.6308 | Acc: 0.8036 | P: 0.5566 | R: 0.6533 | F1: 0.6011 | AUC: 0.8475\nâœ¨ Model saved with F1-score: 0.6011\nğŸ“˜ Epoch 35 | Loss: 0.6264 | Acc: 0.7183 | P: 0.4369 | R: 0.8456 | F1: 0.5762 | AUC: 0.8522\nğŸ“˜ Epoch 36 | Loss: 0.6265 | Acc: 0.7545 | P: 0.4746 | R: 0.7873 | F1: 0.5922 | AUC: 0.8545\nğŸ“˜ Epoch 37 | Loss: 0.6174 | Acc: 0.7547 | P: 0.4752 | R: 0.7970 | F1: 0.5954 | AUC: 0.8533\nğŸ“˜ Epoch 38 | Loss: 0.6101 | Acc: 0.8088 | P: 0.5684 | R: 0.6458 | F1: 0.6047 | AUC: 0.8522\nâœ¨ Model saved with F1-score: 0.6047\nğŸ“˜ Epoch 39 | Loss: 0.6173 | Acc: 0.7298 | P: 0.4492 | R: 0.8553 | F1: 0.5891 | AUC: 0.8600\nğŸ“˜ Epoch 40 | Loss: 0.6118 | Acc: 0.7885 | P: 0.5237 | R: 0.7289 | F1: 0.6095 | AUC: 0.8557\nâœ¨ Model saved with F1-score: 0.6095\nğŸ“˜ Epoch 41 | Loss: 0.6009 | Acc: 0.7855 | P: 0.5184 | R: 0.7473 | F1: 0.6121 | AUC: 0.8595\nâœ¨ Model saved with F1-score: 0.6121\nğŸ“˜ Epoch 42 | Loss: 0.5975 | Acc: 0.7775 | P: 0.5057 | R: 0.7700 | F1: 0.6104 | AUC: 0.8617\nğŸ“˜ Epoch 43 | Loss: 0.5942 | Acc: 0.8009 | P: 0.5450 | R: 0.7322 | F1: 0.6249 | AUC: 0.8640\nâœ¨ Model saved with F1-score: 0.6249\nğŸ“˜ Epoch 44 | Loss: 0.5924 | Acc: 0.7838 | P: 0.5155 | R: 0.7559 | F1: 0.6130 | AUC: 0.8622\nğŸ“˜ Epoch 45 | Loss: 0.5841 | Acc: 0.7403 | P: 0.4593 | R: 0.8283 | F1: 0.5909 | AUC: 0.8653\nğŸ“˜ Epoch 46 | Loss: 0.5828 | Acc: 0.8068 | P: 0.5566 | R: 0.7225 | F1: 0.6288 | AUC: 0.8641\nâœ¨ Model saved with F1-score: 0.6288\nğŸ“˜ Epoch 47 | Loss: 0.5816 | Acc: 0.7457 | P: 0.4653 | R: 0.8261 | F1: 0.5953 | AUC: 0.8679\nğŸ“˜ Epoch 48 | Loss: 0.5796 | Acc: 0.8139 | P: 0.5706 | R: 0.7203 | F1: 0.6368 | AUC: 0.8666\nâœ¨ Model saved with F1-score: 0.6368\nğŸ“˜ Epoch 49 | Loss: 0.5784 | Acc: 0.8046 | P: 0.5513 | R: 0.7376 | F1: 0.6309 | AUC: 0.8691\nğŸ“˜ Epoch 50 | Loss: 0.5765 | Acc: 0.8014 | P: 0.5454 | R: 0.7397 | F1: 0.6279 | AUC: 0.8655\nğŸ“˜ Epoch 51 | Loss: 0.5676 | Acc: 0.7841 | P: 0.5153 | R: 0.7829 | F1: 0.6215 | AUC: 0.8692\nğŸ“˜ Epoch 52 | Loss: 0.5696 | Acc: 0.7789 | P: 0.5077 | R: 0.7829 | F1: 0.6160 | AUC: 0.8690\nğŸ“˜ Epoch 53 | Loss: 0.5635 | Acc: 0.7828 | P: 0.5135 | R: 0.7808 | F1: 0.6195 | AUC: 0.8704\nğŸ“˜ Epoch 54 | Loss: 0.5625 | Acc: 0.7606 | P: 0.4828 | R: 0.8056 | F1: 0.6038 | AUC: 0.8708\nğŸ“˜ Epoch 55 | Loss: 0.5536 | Acc: 0.7863 | P: 0.5187 | R: 0.7775 | F1: 0.6223 | AUC: 0.8697\nğŸ“˜ Epoch 56 | Loss: 0.5588 | Acc: 0.7669 | P: 0.4911 | R: 0.8078 | F1: 0.6109 | AUC: 0.8725\nğŸ“˜ Epoch 57 | Loss: 0.5547 | Acc: 0.7897 | P: 0.5240 | R: 0.7775 | F1: 0.6261 | AUC: 0.8720\nğŸ“˜ Epoch 58 | Loss: 0.5541 | Acc: 0.8156 | P: 0.5740 | R: 0.7203 | F1: 0.6389 | AUC: 0.8706\nâœ¨ Model saved with F1-score: 0.6389\nğŸ“˜ Epoch 59 | Loss: 0.5531 | Acc: 0.8085 | P: 0.5573 | R: 0.7505 | F1: 0.6397 | AUC: 0.8721\nâœ¨ Model saved with F1-score: 0.6397\nğŸ“˜ Epoch 60 | Loss: 0.5492 | Acc: 0.7958 | P: 0.5342 | R: 0.7678 | F1: 0.6300 | AUC: 0.8730\nğŸ“˜ Epoch 61 | Loss: 0.5507 | Acc: 0.8058 | P: 0.5522 | R: 0.7538 | F1: 0.6374 | AUC: 0.8739\nğŸ“˜ Epoch 62 | Loss: 0.5413 | Acc: 0.7745 | P: 0.5013 | R: 0.8045 | F1: 0.6177 | AUC: 0.8742\nğŸ“˜ Epoch 63 | Loss: 0.5431 | Acc: 0.8247 | P: 0.5949 | R: 0.7073 | F1: 0.6463 | AUC: 0.8718\nâœ¨ Model saved with F1-score: 0.6463\nğŸ“˜ Epoch 64 | Loss: 0.5412 | Acc: 0.8068 | P: 0.5534 | R: 0.7613 | F1: 0.6409 | AUC: 0.8758\nğŸ“˜ Epoch 65 | Loss: 0.5373 | Acc: 0.7706 | P: 0.4960 | R: 0.8089 | F1: 0.6149 | AUC: 0.8754\nğŸ“˜ Epoch 66 | Loss: 0.5363 | Acc: 0.8002 | P: 0.5416 | R: 0.7657 | F1: 0.6345 | AUC: 0.8722\nğŸ“˜ Epoch 67 | Loss: 0.5347 | Acc: 0.8225 | P: 0.5871 | R: 0.7279 | F1: 0.6500 | AUC: 0.8744\nâœ¨ Model saved with F1-score: 0.6500\nğŸ“˜ Epoch 68 | Loss: 0.5359 | Acc: 0.8085 | P: 0.5572 | R: 0.7516 | F1: 0.6400 | AUC: 0.8759\nğŸ“˜ Epoch 69 | Loss: 0.5312 | Acc: 0.8156 | P: 0.5736 | R: 0.7235 | F1: 0.6399 | AUC: 0.8724\nğŸ“˜ Epoch 70 | Loss: 0.5348 | Acc: 0.7951 | P: 0.5330 | R: 0.7678 | F1: 0.6292 | AUC: 0.8740\nğŸ“˜ Epoch 71 | Loss: 0.5257 | Acc: 0.8044 | P: 0.5492 | R: 0.7592 | F1: 0.6374 | AUC: 0.8749\nğŸ“˜ Epoch 72 | Loss: 0.5310 | Acc: 0.8112 | P: 0.5617 | R: 0.7570 | F1: 0.6449 | AUC: 0.8740\nğŸ“˜ Epoch 73 | Loss: 0.5218 | Acc: 0.8110 | P: 0.5619 | R: 0.7495 | F1: 0.6423 | AUC: 0.8757\nğŸ“˜ Epoch 74 | Loss: 0.5217 | Acc: 0.8080 | P: 0.5560 | R: 0.7559 | F1: 0.6407 | AUC: 0.8745\nğŸ“˜ Epoch 75 | Loss: 0.5265 | Acc: 0.8237 | P: 0.5919 | R: 0.7127 | F1: 0.6467 | AUC: 0.8738\nğŸ“˜ Epoch 76 | Loss: 0.5222 | Acc: 0.7929 | P: 0.5286 | R: 0.7873 | F1: 0.6325 | AUC: 0.8740\nğŸ“˜ Epoch 77 | Loss: 0.5217 | Acc: 0.8078 | P: 0.5555 | R: 0.7570 | F1: 0.6408 | AUC: 0.8749\nğŸ“˜ Epoch 78 | Loss: 0.5186 | Acc: 0.7508 | P: 0.4718 | R: 0.8391 | F1: 0.6040 | AUC: 0.8765\nğŸ“˜ Epoch 79 | Loss: 0.5219 | Acc: 0.8100 | P: 0.5598 | R: 0.7527 | F1: 0.6421 | AUC: 0.8753\nğŸ›‘ Early stopping triggered.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 6: ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh GAT + Baseline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nimport torch.nn.functional as F\n\n# âœ… Load mÃ´ hÃ¬nh tá»‘t nháº¥t\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\nmodel.eval()\n\n# ÄÃ¡nh giÃ¡\nall_preds, all_probs, all_labels = [], [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        out = model(batch)\n        prob = F.softmax(out, dim=1)\n        pred = torch.argmax(prob, dim=1)\n\n        all_preds.extend(pred.cpu().tolist())\n        all_probs.extend(prob[:, 1].cpu().tolist())  # class = 1\n        all_labels.extend(batch.y.cpu().tolist())\n\n# ğŸ¯ Káº¿t quáº£ trÃªn táº­p test\nprint(\"ğŸ“Š ÄÃ¡nh giÃ¡ trÃªn táº­p test:\")\nprint(f\"Accuracy : {accuracy_score(all_labels, all_preds):.4f}\")\nprint(f\"Precision: {precision_score(all_labels, all_preds):.4f}\")\nprint(f\"Recall   : {recall_score(all_labels, all_preds):.4f}\")\nprint(f\"F1-score : {f1_score(all_labels, all_preds):.4f}\")\ntry:\n    print(f\"AUC-ROC  : {roc_auc_score(all_labels, all_probs):.4f}\")\nexcept:\n    print(\"âš ï¸ KhÃ´ng thá»ƒ tÃ­nh AUC-ROC (dá»¯ liá»‡u chá»‰ chá»©a 1 class)\")\n\nprint(\"\\nğŸ“Œ BÃ¡o cÃ¡o chi tiáº¿t:\")\nprint(classification_report(all_labels, all_preds, digits=4))\n\n# ğŸ¯ Baseline: Logistic Regression\nX_train = torch.stack([d.x.mean(dim=0) for d in train_data]).numpy()\ny_train = [d.y.item() for d in train_data]\nX_test = torch.stack([d.x.mean(dim=0) for d in test_data]).numpy()\ny_test = [d.y.item() for d in test_data]\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    baseline = LogisticRegression(max_iter=1000)\n    baseline.fit(X_train_scaled, y_train)\n\nbaseline_preds = baseline.predict(X_test_scaled)\nbaseline_f1 = f1_score(y_test, baseline_preds)\n\nprint(f\"\\nğŸ“‰ Baseline F1-score (Logistic Regression): {baseline_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:40:03.602741Z","iopub.execute_input":"2025-07-10T10:40:03.603264Z","iopub.status.idle":"2025-07-10T10:40:22.588600Z","shell.execute_reply.started":"2025-07-10T10:40:03.603243Z","shell.execute_reply":"2025-07-10T10:40:22.585601Z"}},"outputs":[{"name":"stdout","text":"ğŸ“Š ÄÃ¡nh giÃ¡ trÃªn táº­p test:\nAccuracy : 0.8074\nPrecision: 0.5630\nRecall   : 0.7184\nF1-score : 0.6313\nAUC-ROC  : 0.8741\n\nğŸ“Œ BÃ¡o cÃ¡o chi tiáº¿t:\n              precision    recall  f1-score   support\n\n           0     0.9086    0.8339    0.8696      3112\n           1     0.5630    0.7184    0.6313       927\n\n    accuracy                         0.8074      4039\n   macro avg     0.7358    0.7762    0.7505      4039\nweighted avg     0.8293    0.8074    0.8149      4039\n\n\nğŸ“‰ Baseline F1-score (Logistic Regression): 0.4671\n","output_type":"stream"}],"execution_count":17}]}